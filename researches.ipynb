{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.models import LSTM, DilatedRNN\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_path = '/content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contract_price_by_market_month(dir_path,market_month:str):\n",
    "    try: \n",
    "        # file_path =\"./storage/contract_data.csv\"\n",
    "        file_name = \"contract_data.csv\"\n",
    "        # file_path =\"../storage/contract_data.csv\"\n",
    "        check_file = os.path.isfile(os.path.join(dir_path, file_name))\n",
    "        print(check_file)\n",
    "        print(os.path.join(dir_path, file_name))\n",
    "        if check_file:\n",
    "            temp = pd.read_csv(os.path.join(dir_path, file_name), parse_dates=[\"data_date\"])\n",
    "            market_month_list = temp[\"market_month\"].unique()\n",
    "            if market_month in market_month_list:\n",
    "                df = temp[temp[\"market_month\"]==market_month]\n",
    "                return df\n",
    "            else:\n",
    "                raise ValueError('Invalid market month')\n",
    "        else:\n",
    "            return ValueError('No file exist')\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_exogenous_data(dir_path):\n",
    "    file = \"futures_notcontract_soybean.xlsx\"\n",
    "    path = os.path.join(dir_path, file)\n",
    "    futures = pd.read_excel(path)\n",
    "\n",
    "    # % of missing value by rows were dropped\n",
    "    missing_values_percentage = futures.isnull().sum(axis=1) / len(futures.columns) * 100\n",
    "    rows_to_drop = missing_values_percentage[missing_values_percentage > 5].index\n",
    "    cleaned_df = futures.drop(index=rows_to_drop)\n",
    "    cleaned_df = cleaned_df.set_index(\"data_date\").sort_index() # set datetime column to index and sort\n",
    "    cleaned_df2 = cleaned_df.drop(columns=[\"usd_idx_price\", \"usd_idx_low\", \"usd_idx_hight\", \"selling\"]) # drop usd index bc high number of missing value\n",
    "\n",
    "    missing_values_percentage = cleaned_df2.isnull().sum() / len(cleaned_df2) * 100\n",
    "\n",
    "    # drop column remained missing values which is more than 10% \n",
    "    # Identify columns where the percentage of missing values exceeds 10%\n",
    "    columns_to_drop = missing_values_percentage[missing_values_percentage > 10].index\n",
    "    cleaned_df2 = cleaned_df2.drop(columns=columns_to_drop)\n",
    "\n",
    "    # load new usdx data\n",
    "    usdx1_file = \"usdx_2020.csv\"\n",
    "    usdx2_file = \"usdx_2021.csv\"\n",
    "    usdx3_file = \"usdx_2022.csv\"\n",
    "    usdx4_file = \"usdx_2023.csv\"\n",
    "    usdx5_file = \"usdx_2024.csv\"\n",
    "    usdx1 = pd.read_csv(os.path.join(dir_path, usdx1_file), parse_dates=[\"Date\"])\n",
    "    usdx2 = pd.read_csv(os.path.join(dir_path, usdx2_file), parse_dates=[\"Date\"])\n",
    "    usdx3 = pd.read_csv(os.path.join(dir_path, usdx3_file), parse_dates=[\"Date\"])\n",
    "    usdx4 = pd.read_csv(os.path.join(dir_path, usdx4_file), parse_dates=[\"Date\"])\n",
    "    usdx5 = pd.read_csv(os.path.join(dir_path, usdx5_file), parse_dates=[\"Date\"])\n",
    "\n",
    "    usdx1 = usdx1.set_index(\"Date\").sort_index()\n",
    "    usdx2 = usdx2.set_index(\"Date\").sort_index()\n",
    "    usdx3 = usdx3.set_index(\"Date\").sort_index()\n",
    "    usdx4 = usdx4.set_index(\"Date\").sort_index()\n",
    "    usdx5 = usdx5.set_index(\"Date\").sort_index()\n",
    "\n",
    "    usdx = pd.concat([usdx1, usdx2, usdx3, usdx4, usdx5])\n",
    "    usdx = usdx.drop(columns=[\"Open\", \"High\", \"Low\"]) # use only closed price of usdx\n",
    "\n",
    "    # Merge with new usd index data\n",
    "    merged_df = pd.concat([cleaned_df2, usdx], axis=1)\n",
    "    merged_df = merged_df.dropna() # drop na after merging\n",
    "\n",
    "    # include only global value columns\n",
    "    new_df1 = merged_df.copy()\n",
    "    new_df2 = merged_df.copy()\n",
    "    val_tot_columns = [col for col in merged_df.columns if 'value_'  in col.lower()]\n",
    "    global_columns = [col for col in merged_df.columns if 'value_total'  in col.lower()]\n",
    "    global_demand_sup = new_df2[global_columns]\n",
    "    new_df1 = new_df1.drop(columns=val_tot_columns)\n",
    "    merged_df2 = pd.concat([new_df1, global_demand_sup], axis=1)\n",
    "    \n",
    "\n",
    "    # exclude columns which has low correlation (<0.4 and >-0.4)\n",
    "    exclude_feat_list = []\n",
    "    cif_columns = [col for col in merged_df.columns if 'cif'  in col.lower()]\n",
    "    vol_columns = [col for col in merged_df.columns if 'volume'  in col.lower()]\n",
    "    qty_col = [col for col in merged_df.columns if 'quntity'  in col.lower()]\n",
    "\n",
    "    group_feat = [cif_columns, vol_columns, qty_col]\n",
    "    for feat in group_feat:\n",
    "        # print(\"column:  \", feat)\n",
    "        fil_col = exclude_feature(merged_df2,feat)\n",
    "        exclude_feat_list.extend(fil_col)\n",
    "\n",
    "    cleaned = merged_df2.drop(columns=exclude_feat_list)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "def get_corr_rank(merged_df, feature_list: list, target=\"price_soybean-seed\"):   \n",
    "    feature_list.append(target)\n",
    "    corr_rank = merged_df[feature_list].corr()[target]\n",
    "    return corr_rank.sort_values()\n",
    "\n",
    "def exclude_feature(merged_df, feature_list: list,target=\"price_soybean-seed\", corr_val=0.4):\n",
    "    corr_rank = get_corr_rank(merged_df,feature_list, target)\n",
    "    # print(corr_rank)\n",
    "    filter_corr = corr_rank[(corr_rank <= corr_val) & (corr_rank >= -corr_val)].index.tolist()\n",
    "    return filter_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "  \n",
    "  data = df.rename(columns={\"Unnamed: 0\":\"ds\", \"price_settle\":\"y\"})\n",
    "  data2 = data.copy()\n",
    "\n",
    "  # print(\"column ds \",data2[\"ds\"])\n",
    "\n",
    "  # moving average\n",
    "  ma30 = data2[\"y\"].rolling(window=30, center=True, min_periods=15).mean()\n",
    "  data2 = data2.drop(columns=\"y\") # drop old column y\n",
    "  data2[\"y\"] = ma30 # create new column y (trend MA30)\n",
    "  \n",
    "  data2[\"unique_id\"] = \"H1\"\n",
    "\n",
    "\n",
    "  return data2\n",
    "\n",
    "def get_exo_columns(df):\n",
    "\n",
    "  exo_var = df.drop(columns=[\"ds\",\"y\",\"unique_id\"])\n",
    "  exo_col = exo_var.columns.tolist()\n",
    "\n",
    "  return exo_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dir_path):\n",
    "  file_mkt_month = \"market_month_to_train.csv\"\n",
    "  market_month_to_train = pd.read_csv(os.path.join(dir_path, file_mkt_month))\n",
    "#   storage_path = \"./storage\"\n",
    "  exo_data = ingest_exogenous_data(dir_path)\n",
    "  print(exo_data)\n",
    "  all_data = []\n",
    "  for market_month in market_month_to_train[\"market_month\"]:\n",
    "      # self.market_month = market_month\n",
    "      contract_price = get_contract_price_by_market_month(dir_path=dir_path, market_month=market_month)\n",
    "      contract_price = contract_price.drop_duplicates(subset=[\"data_date\"])\n",
    "      contract_price = contract_price.groupby(\"data_date\")[\"price_settle\"].sum().reset_index()\n",
    "      # print(contract_price.head())\n",
    "\n",
    "      if contract_price[\"price_settle\"].isna().sum() > 0:\n",
    "          contract_price.dropna(axis=0, inplace=True)\n",
    "\n",
    "      # print(contract_price)\n",
    "      break\n",
    "\n",
    "      # all_data.append(contract_price)\n",
    "      exo_data1 = exo_data.copy()\n",
    "      exo_data1 = exo_data1.drop(columns=\"price_soybean-seed\")\n",
    "\n",
    "      cleaned_contract = pd.merge(exo_data, contract_price, left_index=True, right_index=True)\n",
    "\n",
    "      # train data\n",
    "\n",
    "      \n",
    "\n",
    "      all_data.append(cleaned_contract)\n",
    "  return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "./storage/contract_data.csv\n",
      "     data_date  price_settle\n",
      "0   2021-11-12       1182.75\n",
      "1   2021-11-15       1190.00\n",
      "2   2021-11-16       1187.25\n",
      "3   2021-11-17       1191.00\n",
      "4   2021-11-18       1175.75\n",
      "..         ...           ...\n",
      "563 2024-03-20       1219.40\n",
      "564 2024-03-21       1223.00\n",
      "565 2024-03-22       1202.60\n",
      "566 2024-03-25       1218.20\n",
      "567 2024-03-26       1209.60\n",
      "\n",
      "[568 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"./storage\"\n",
    "# ingest_exogenous_data(dir_path=dir_path)\n",
    "all_data = train(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('neuralforecast': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4a775e2ffe3b955e922db43dcf14003b67383e63ff8eb8d77fb8e0d2506f658"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
